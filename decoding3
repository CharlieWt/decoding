import scipy.io as sio
from numpy import *
from keras.models import Model
from keras.layers import Dense, Flatten, Input, merge, Conv2D, Lambda,Conv1D, Reshape,Add, \
LeakyReLU, Dropout,BatchNormalization,GlobalAveragePooling2D,GaussianNoise,GaussianDropout
from keras.optimizers import *
from keras.constraints import *
from keras import backend as K
from keras import regularizers
from keras import initializers
import h5py
import os
import numpy as np
from keras.applications.vgg16 import VGG16
import matplotlib.pyplot as plt
from keras.utils.generic_utils import Progbar
def perceptual_loss(y_true, y_pred):
    x=K.concat(y_true,y_true,axis=-1)
    x = K.concat(x, y_true, axis=-1)
    y=K.concat(y_pred,y_pred,axis=-1)
    y = K.concat(y, y_pred, axis=-1)
    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(96,96,3))
    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)
    loss_model.trainable = False
    return K.mean(K.square(loss_model(x) - loss_model(y)))

os.environ["CUDA_VISIBLE_DEVICES"] = "0"

def l1_reg(weight_matrix):
    return 1


load_fn1 = h5py.File(r'/home/amax/Desktop/train_pic.mat')
D1 = load_fn1['pic'][:]
D1 = D1.transpose((2,1,0))
D1 = np.expand_dims(D1,axis=-1)
D1 =(D1-127.5)/127.5
print(D1.shape)



test_pic = sio.loadmat('test_pic.mat')['test_pic']
test_pic =(test_pic-127.5)/127.5
#test_signal = test_signal/max(abs(test_signal[:]))/1.2
#print(test_signal)
expend = Lambda(lambda x: K.expand_dims(x, -1))
aclrt = Lambda(lambda x: K.sum(K.sum(x, 1), 1))
round = Lambda(lambda x: K.round(x))


x_test = np.zeros((1000, 96, 96, 1))

y_test = np.zeros((1000, 96, 96, 1))
x_train, y_train = D1, D1
x_test[:, :, :, 0], y_test[:, :, :, 0] = test_pic , test_pic



def wasserstein_loss(y_true, y_pred):
    return K.mean(y_true*y_pred)
'''
class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []

    def on_epoch_end(self, epoch, logs={}):
        self.losses.append(logs.get('loss'))


class ValueLoss(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.vallos = []

    def on_epoch_end(self, epoch, logs={}):
        self.vallos.append(logs.get('val_loss'))
'''

sum_array = Lambda(lambda x: K.sum(x[:]))
M = 1000


class mydecodenet(object):
    def __init__(self):

        self.GM = None
        self.MN = None
        self.DM = None
    def get_net(self):
        inputs = Input((96, 96, 1))
        flt = Flatten()(inputs)
        dense0 = Dense(M, use_bias=False,kernel_regularizer=regularizers.lb(1e-6),name='weight1')(flt)
        dense1 = Dense(2 * M,activation='tanh')(dense0)
        dense3 = Dense(96 * 96)(dense1)
        print(dense3.shape)
        #dense3 = BatchNormalization()(dense3)
        dense4 = Reshape((96, 96, 1), input_shape=(96 * 96,))(dense3)
        #print('dense4', dense5.shape)
        Conv1 = Conv2D(64, (5, 5), strides=(1, 1), padding='same',kernel_initializer='random_uniform')(dense4)
        Conv1 = BatchNormalization()(Conv1)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        Conv1 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',kernel_initializer='random_uniform')(Conv1)
        #print('Conv1', Conv1.shape)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        Conv1 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv1)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        Conv1 = Dropout(0.7)(Conv1)

        #BN2 = BatchNormalization()(Conv1)
        merge0 = merge([Conv1,dense4],mode='concat',concat_axis=-1)

        Conv2 = Conv2D(64, (3, 3), strides=(1, 1), padding='same',kernel_initializer='random_uniform')(merge0)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        Conv2 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv2)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        Conv2 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv2)
        Conv2 = Dropout(0.7)(Conv2)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        #print('Conv2', Conv2.shape)
        #BN3 = BatchNormalization()(Conv2)
        merge1 = merge([Conv2,dense4, merge0],mode='concat',concat_axis=-1)

        Conv3 = Conv2D(64, (3, 3), strides=(1, 1), padding='same',kernel_initializer='random_uniform')(merge1)
        #BN4 = BatchNormalization()(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Dropout(0.7)(Conv3)
        merge2 =  merge([Conv3,dense4, merge1],mode='concat',concat_axis=-1)
        Conv3 = Conv2D(1, (3, 3), activation='tanh', strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(merge2)
        #print('Conv3', Conv3.shape)
        dense0 = expend(dense0)
        print('dense2', dense0.shape)
        model = Model(input=inputs, output=[Conv3, dense0])
        #model.load_weights('mask1000.h5',by_name=True)
        model.compile(optimizer=Adam(lr=1e-4), loss='great_error96')
        return model

    def diff_net(self):
        input = Input((M,1))

        flt = Flatten()(input)
        dense0 = Dense(M)(flt)
        dense1 = Dense(2*M)(dense0)
        dense3 = Dense(96 * 96)(dense1)
        # dense3 = BatchNormalization()(dense3)
        dense4 = Reshape((96, 96, 1), input_shape=(96 * 96,))(dense3)
        # print('dense4', dense5.shape)
        Conv1 = Conv2D(64, (5, 5), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(dense4)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        #drop1 = Dropout(0.2)(Conv1)
        # BN1 = BatchNormalization()(Conv1)
        Conv1 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv1)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        # print('Conv1', Conv1.shape)
        Conv1 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv1)
        Conv1 = Dropout(0.7)(Conv1)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        # BN2 = BatchNormalization()(Conv1)
        merge0 = merge([Conv1, dense4], mode='concat', concat_axis=-1)

        Conv2 = Conv2D(64, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(merge0)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        Conv2 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv2)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        Conv2 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv2)
        Conv2 = Dropout(0.7)(Conv2)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        # print('Conv2', Conv2.shape)
        # BN3 = BatchNormalization()(Conv2)
        merge1 = merge([Conv2, dense4, merge0], mode='concat', concat_axis=-1)


        Conv3 = Conv2D(64, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(merge1)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        # BN4 = BatchNormalization()(Conv3)
        Conv3 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Dropout(0.7)(Conv3)
        merge2 = merge([Conv3, dense4, merge1], mode='concat', concat_axis=-1)
        Conv3 = Conv2D(1, (3, 3), activation='linear', strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(merge2)
        # print('Conv3', Conv3.shape)
        model = Model(input=input, output=Conv3)
        #model.load_weights('diff_v4.h5',by_name=True)
        model.compile(optimizer=Adam(lr=1e-4), loss='great_error96')
        model.save_weights('diff_v4.h5')
        return model

    def dis_net(self, trainable):
        inputs = Input((96,96,1))
        conv1 = Conv2D(3, 5, strides=2,
                       padding='same')(inputs)
        Relu1 = LeakyReLU(alpha=0.2)(conv1)
        drop1 = Dropout(0.7)(Relu1)

        conv2 = Conv2D(3 * 1, 5, strides=2, padding='same')(drop1)
        Relu2 = LeakyReLU(alpha=0.2)(conv2)
        drop2 = Dropout(0.5)(Relu2)

        conv3 = Conv2D(3 * 1, 5, strides=2, padding='same')(drop2)
        Relu3 = LeakyReLU(alpha=0.2)(conv3)
        drop3 = Dropout(0.5)(Relu3)

        conv4 = Conv2D(3 * 1, 5, strides=1, padding='same')(drop3)
        Relu4 = LeakyReLU(alpha=0.2)(conv4)
        drop4 = Dropout(0.5)(Relu4)
        flt = Flatten()(drop4)
        dense1 = Dense(1, activation='linear')(flt)
        model = Model(input=inputs, output=dense1)
        model.trainable = trainable
        return model

    def generate_model(self):
        optimizer = Adam(lr=0.0001, decay=3e-8)
        input = Input((96,96,1))
        [output,feature] = self.get_net()(input)
        model = Model(inputs=input,outputs=output)
        model.compile(loss='great_error96', optimizer=optimizer)
        #self.GM.save('GMs_v3.h5')
        return model

    def diff_model(self):
        optimizer = Adam(lr=0.0001, decay=3e-8)
        input = Input((96,96,1))
        [output, feature] = self.get_net()(input)
        diff = self.diff_net()(feature)
        model = Model(inputs=input, outputs=diff)
        model.compile(loss='great_error96', optimizer=optimizer)
        return model

    def Merge_model(self):
        input = Input((96,96,1))
        [output, feature] = self.get_net()(input)
        diff = self.diff_net()(feature)
        merge1 = Add()([output,diff])
        model = Model(input=input, output=merge1)
        model.compile(optimizer=Adam(lr=1e-4), loss='great_error96')
        model.save('mergenet.h5')
        return model

class MNIST_DCGAN(object):
    def __init__(self):
        self.img_rows = 96
        self.img_cols = 96
        self.channel = 1
        self.Codenet = mydecodenet()
        self.autocode = self.Codenet.get_net()
        self.generator = self.Codenet.generate_model()
        #self.masker = self.Codenet.mask_net()
        self.diffnet = self.Codenet.diff_net()
        self.diffmodel = self.Codenet.diff_model()
        self.mergor = self.Codenet.Merge_model()
        self.dis_net = self.Codenet.dis_net(trainable=False)

    def train(self, train_steps=100, batch_size=100, save_interval=0):
        noise_input = None
        if save_interval > 0:
            noise_input = x_train[np.random.randint(0, 4000, size=(1)), :, :, :]
        A = np.zeros((batch_size, 96,96,1))
        for i in range(train_steps):
            batch = np.random.randint(0, 100000, size=batch_size)
            input = x_train[batch, :, :, :]
            [pic, real_mask] = self.autocode.predict(input)
            if i >=2:
                A = self.diffnet.predict(real_mask)
            y = input-A
            g_loss = self.generator.train_on_batch(input, y)


            batch = np.random.randint(0, 100000, size=batch_size)
            input = x_train[batch, :, :, :]
            [pic, real_mask] = self.autocode.predict(input)
            diff = input - pic
            d_loss = self.diffnet.train_on_batch(real_mask, diff)

            batch = np.random.randint(0, 100000, size=batch_size)
            input = x_train[batch, :, :, :]
            m_loss = self.mergor.train_on_batch(input,input)



            #[pic, real_mask] = self.autocode.predict(input)
            #A = self.diffnet.predict(real_mask)
            log_mesg = "%d: [g loss: %f]" % (i, g_loss)
            log_mesg = "%s  [D loss: %f]" % (log_mesg, d_loss)
            log_mesg = "%s  [M loss: %f]" % (log_mesg, m_loss)
            print(log_mesg)



        print('predict test data')
        Vt_result = self.generator.predict(x_test)
        print(Vt_result)
        sio.savemat('test_dpm_gan_v6.mat', {'result': Vt_result})

        [pic, real_mask] = self.autocode.predict(x_test)
        diff_result = self.diffnet.predict(real_mask)
        sio.savemat('test_dpm_gan_v6_diff.mat', {'result': diff_result})
        final_result = self.mergor.predict(x_test)
        sio.savemat('test_dpm_gan_v6_final.mat', {'result': final_result})
        self.mergor.save('mergemodel.h5')
        self.mergor.save_weights('mergemodelweights.h5')

if __name__ == '__main__':
    mnist_dcgan = MNIST_DCGAN()
    mnist_dcgan.train(train_steps=5000, batch_size=100, save_interval=500)
