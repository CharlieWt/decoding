import scipy.io as sio
from numpy import *
from keras.models import Model
from keras.layers import Dense, Flatten, Input, merge, Conv2D, Lambda,Conv1D, Reshape,Add, \
LeakyReLU, Dropout,BatchNormalization,UpSampling2D,GlobalAveragePooling2D
from keras.optimizers import *
from keras.constraints import *
from keras import backend as K
from keras import regularizers
from keras import initializers
from keras.utils import multi_gpu_model
import h5py
import os
import numpy as np
from keras.applications.vgg16 import VGG16
import matplotlib.pyplot as plt
import os
import tensorflow as tf
import keras.backend.tensorflow_backend as KTF
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.Session(config=config)
KTF.set_session(sess)


def l1_reg(weight_matrix):
    return 1


load_fn1 = h5py.File(r'/home/amax/Desktop/train_pic128_1.mat')
D1 = load_fn1['pic128'][:]
D1 = D1.transpose((2,1,0))
D1=(D1-127.5)/127.5
x_train = np.expand_dims(D1,axis=-1)
print(x_train.shape)

load_fn2 = h5py.File(r'/home/amax/Desktop/train_pic128_2.mat')
D2 = load_fn2['pic128'][:]
D2 = D2.transpose((2,1,0))
D2=(D2-127.5)/127.5
x_test = np.expand_dims(D2,axis=-1)
print(x_test.shape)

load_fn3 = h5py.File(r'/home/amax/Desktop/train_pic256_1.mat')
D3 = load_fn3['pic256'][:]
D3 = D3.transpose((2,1,0))
D3=(D3-127.5)/127.5
y_train = np.expand_dims(D3,axis=-1)
print(x_train.shape)

load_fn4 = h5py.File(r'/home/amax/Desktop/train_pic256_2.mat')
D4 = load_fn4['pic256'][:]
D4 = D4.transpose((2,1,0))
D4=(D4-127.5)/127.5
y_test = np.expand_dims(D4,axis=-1)
print(x_test.shape)

#test_signal = test_signal/max(abs(test_signal[:]))/1.2
#print(test_signal)
expend = Lambda(lambda x: K.expand_dims(x, -1))
aclrt = Lambda(lambda x: K.sum(K.sum(x, 1), 1))
round = Lambda(lambda x: K.round(x))




def wasserstein_loss(y_true, y_pred):
    return K.mean(y_true*y_pred)
'''
class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []

    def on_epoch_end(self, epoch, logs={}):
        self.losses.append(logs.get('loss'))


class ValueLoss(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.vallos = []

    def on_epoch_end(self, epoch, logs={}):
        self.vallos.append(logs.get('val_loss'))
'''

sum_array = Lambda(lambda x: K.sum(x[:]))
M = 3000

class mydecodenet(object):
    def __init__(self):

        self.GM = None
        self.MN = None
        self.DM = None
    def get_net(self):
        inputs = Input((128, 128, 1))
        flt = Flatten()(inputs)
        dense0 = Dense(M, use_bias=False,kernel_regularizer=regularizers.lb(1e-8))(flt)
        #dense1 = Dense(2 * M,activation='tanh')(dense0)
        dense3 = Dense(128 * 128)(dense0)
        print(dense3.shape)
        #dense3 = BatchNormalization()(dense3)
        dense4 = Reshape((128, 128, 1), input_shape=(128 * 128,))(dense3)
        #print('dense4', dense5.shape)
        Conv1 = Conv2D(64, (5, 5), strides=(1, 1), padding='same',kernel_initializer='random_uniform')(dense4)
        Conv1 = BatchNormalization()(Conv1)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        Conv1 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',kernel_initializer='random_uniform')(Conv1)
        #print('Conv1', Conv1.shape)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        Conv1 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv1)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        Conv1 = Dropout(0.7)(Conv1)

        #BN2 = BatchNormalization()(Conv1)
        merge0 = merge([Conv1,dense4],mode='concat',concat_axis=-1)

        Conv2 = Conv2D(64, (3, 3), strides=(1, 1), padding='same',kernel_initializer='random_uniform')(merge0)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        Conv2 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv2)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        Conv2 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv2)
        Conv2 = Dropout(0.7)(Conv2)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        #print('Conv2', Conv2.shape)
        #BN3 = BatchNormalization()(Conv2)
        merge1 = merge([Conv2,dense4, merge0],mode='concat',concat_axis=-1)

        Conv3 = Conv2D(64, (3, 3), strides=(1, 1), padding='same',kernel_initializer='random_uniform')(merge1)
        #BN4 = BatchNormalization()(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Dropout(0.7)(Conv3)
        merge2 =  merge([Conv3,dense4, merge1],mode='concat',concat_axis=-1)
        Conv3 = Conv2D(1, (3, 3), activation='tanh', strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(UpSampling2D(size=(2, 2))(merge2))
        #print('Conv3', Conv3.shape)
        dense0 = expend(dense0)
        print('dense2', dense0.shape)
        model = Model(input=inputs, output=[Conv3, dense0])
        #model.load_weights('mask1000.h5',by_name=True)
        model.compile(optimizer=Adam(lr=1e-4), loss='ase_error')
        model.save_weights('mask1000.h5')
        model.save('get_mask1.h5')
        return model

    def diff_net(self):
        input = Input((M,1))

        flt = Flatten()(input)
        dense0 = Dense(M)(flt)
        #dense1 = Dense(2*M)(dense0)
        dense3 = Dense(128 * 128)(dense0)
        # dense3 = BatchNormalization()(dense3)
        dense4 = Reshape((128, 128, 1), input_shape=(128 * 128,))(dense3)
        # print('dense4', dense5.shape)
        Conv1 = Conv2D(64, (5, 5), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(dense4)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        #drop1 = Dropout(0.2)(Conv1)
        # BN1 = BatchNormalization()(Conv1)
        Conv1 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv1)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        # print('Conv1', Conv1.shape)
        Conv1 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv1)
        Conv1 = Dropout(0.7)(Conv1)
        #Conv1 = LeakyReLU(alpha=0.2)(Conv1)
        # BN2 = BatchNormalization()(Conv1)
        merge0 = merge([Conv1, dense4], mode='concat', concat_axis=-1)

        Conv2 = Conv2D(64, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(merge0)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        Conv2 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv2)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        Conv2 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv2)
        Conv2 = Dropout(0.7)(Conv2)
        #Conv2 = LeakyReLU(alpha=0.2)(Conv2)
        # print('Conv2', Conv2.shape)
        # BN3 = BatchNormalization()(Conv2)
        merge1 = merge([Conv2, dense4, merge0], mode='concat', concat_axis=-1)


        Conv3 = Conv2D(64, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(merge1)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        # BN4 = BatchNormalization()(Conv3)
        Conv3 = Conv2D(32, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Conv2D(1, (3, 3), strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(Conv3)
        #Conv3 = LeakyReLU(alpha=0.2)(Conv3)
        Conv3 = Dropout(0.7)(Conv3)
        merge2 = merge([Conv3, dense4, merge1], mode='concat', concat_axis=-1)
        Conv3 = Conv2D(1, (3, 3), activation='linear', strides=(1, 1), padding='same',
                       kernel_initializer='random_uniform')(UpSampling2D(size=(2, 2))(merge2))
        # print('Conv3', Conv3.shape)
        model = Model(input=input, output=Conv3)
        #model.load_weights('diff_v4.h5',by_name=True)
        model.compile(optimizer=Adam(lr=1e-4), loss='ase_error')
        model.save_weights('diff_v4.h5')
        return model

    def dis_net(self, trainable):
        inputs = Input((256,256,1))
        conv1 = Conv2D(3, 5, strides=2,
                       padding='same')(inputs)
        Relu1 = LeakyReLU(alpha=0.2)(conv1)
        drop1 = Dropout(0.7)(Relu1)

        conv2 = Conv2D(3 * 1, 5, strides=2, padding='same')(drop1)
        Relu2 = LeakyReLU(alpha=0.2)(conv2)
        drop2 = Dropout(0.5)(Relu2)

        conv3 = Conv2D(3 * 1, 5, strides=2, padding='same')(drop2)
        Relu3 = LeakyReLU(alpha=0.2)(conv3)
        drop3 = Dropout(0.5)(Relu3)

        conv4 = Conv2D(3 * 1, 5, strides=1, padding='same')(drop3)
        Relu4 = LeakyReLU(alpha=0.2)(conv4)
        drop4 = Dropout(0.5)(Relu4)
        flt = Flatten()(drop4)
        dense1 = Dense(1, activation='linear')(flt)
        model = Model(input=inputs, output=dense1)
        model.trainable = trainable
        for l in model.layers: l.trainable = trainable
        return model

    def generate_model(self):
        optimizer = Adam(lr=0.0001, decay=3e-8)
        input = Input((128,128,1))
        [output,feature] = self.get_net()(input)
        model = Model(inputs=input,outputs=output)
        model.compile(loss='ase_error', optimizer=optimizer)
        #self.GM.save('GMs_v3.h5')
        return model

    def diff_model(self):
        optimizer = Adam(lr=0.0001, decay=3e-8)
        input = Input((128,128,1))
        [output, feature] = self.get_net()(input)
        diff = self.diff_net()(feature)
        model = Model(inputs=input, outputs=diff)
        model.compile(loss='ase_error', optimizer=optimizer)
        return model

    def Merge_model(self):
        input = Input((128,128,1))
        [output, feature] = self.get_net()(input)
        diff = self.diff_net()(feature)
        merge1 = Add()([output,diff])

        model = Model(input=input, output=merge1)
        model.compile(optimizer=Adam(lr=1e-4), loss='great_error')
        model.save('mergenet.h5')
        return model

class MNIST_DCGAN(object):
    def __init__(self):
        self.img_rows = 96
        self.img_cols = 96
        self.channel = 1
        self.Codenet = mydecodenet()
        self.autocode = self.Codenet.get_net()
        self.generator = self.Codenet.generate_model()
        #self.masker = self.Codenet.mask_net()
        self.diffnet = self.Codenet.diff_net()
        self.diffmodel = self.Codenet.diff_model()
        self.mergor = self.Codenet.Merge_model()
        self.dis_net = self.Codenet.dis_net(trainable=False)

    def train(self, train_steps=100, batch_size=100, save_interval=0):
        noise_input = None
        if save_interval > 0:
            noise_input = x_train[np.random.randint(0, 4000, size=(1)), :, :, :]
        A = np.zeros((batch_size, 96,96,1))
        for i in range(train_steps):
            batch = np.random.randint(0, 10000, size=batch_size)
            input = x_train[batch, :, :, :]
            GT = y_train[batch, :, :, :]
            [pic, real_mask] = self.autocode.predict(input)

            A = self.diffnet.predict(real_mask)
            y = GT-A
            g_loss = self.generator.train_on_batch(input, y)


            batch = np.random.randint(0, 10000, size=batch_size)
            input = x_train[batch, :, :, :]
            GT = y_train[batch, :, :, :]
            [pic, real_mask] = self.autocode.predict(input)
            diff = GT - pic
            d_loss = self.diffnet.train_on_batch(real_mask, diff)

            batch = np.random.randint(0, 10000, size=batch_size)
            input = x_train[batch, :, :, :]
            GT = y_train[batch, :, :, :]
            m_loss = self.mergor.train_on_batch(input,GT)



            #[pic, real_mask] = self.autocode.predict(input)
            #A = self.diffnet.predict(real_mask)
            log_mesg = "%d: [g loss: %f]" % (i, g_loss)
            log_mesg = "%s  [D loss: %f]" % (log_mesg, d_loss)
            log_mesg = "%s  [M loss: %f]" % (log_mesg, m_loss)
            print(log_mesg)



        print('predict test data')
        final_result = self.mergor.predict(x_test,batch_size=32)
        sio.savemat('test_dpm_gan_v6_final.mat', {'result': final_result})
        self.mergor.save('merge256.h5')
        self.mergor.save_weights('merge256weights.h5')

if __name__ == '__main__':
    mnist_dcgan = MNIST_DCGAN()
    mnist_dcgan.train(train_steps=5000, batch_size=32, save_interval=500)
